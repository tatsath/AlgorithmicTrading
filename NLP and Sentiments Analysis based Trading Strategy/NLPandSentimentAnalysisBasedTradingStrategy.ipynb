{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83708667-4fdc-1563-7b3a-06b6575d2865"
   },
   "source": [
    "# NLP and Sentiment Analysis based Trading Strategy\n",
    "\n",
    "In this case study we use NLP to build a trading strategy combining some of the concepts that we went through in some of the previous chapters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Problem Statement](#1)\n",
    "* [2. Getting Started - Load Libraries and Dataset](#2)\n",
    "    * [2.1. Load Libraries](#2.1) \n",
    "    * [2.1. Loading the stock data](#2.2)  \n",
    "* [3. Data Preparation](#3)      \n",
    "    * [3.1. Loading and preprocessing the News data](#3.1)\n",
    "    * [3.2. Preparing the combined data](#3.2)\n",
    "    * [3.3. Preparing combined data](#3.3)\n",
    "* [4.Model Evaluation-Sentiment Analysis](#4)    \n",
    "    * [4.1. Predefined model-TextBlob package](#4.1)\n",
    "    * [4.2. Supervised Learning based-Classification algorithms and LSTM](#4.2)\n",
    "    * [4.3. Unsupervised Learning based-based on financial lexicon](#4.3)  \n",
    "    * [4.4. Exploratory Data Analysis and comparison](#4.4)  \n",
    "* [5.Models Evaluation-Building a Trading Strategy ](#5)\n",
    "    * [5.1 Setting up the strategy ](#5.1) \n",
    "    * [5.2.Results by Individual Stocks](#5.2) \n",
    "    * [5.3.Results by Multiple Stocks](#5.3) \n",
    "    * [5.4 Results by Varying Time Period](#5.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem statement is to build a trading strategy that uses NLP to extracts the information inside the news headlines, assigns a sentiment to that and use the sentiments and the information inside the news headlines for a trading strategy.\n",
    "\n",
    "The data used for this case study will be from three sources:\n",
    "* **News headlines data compiled from RSS feeds of several news websites**: This news headlines data is complied by different news website and contains the most financially relevant news filtered by human editors. For the purpose of this study, we will only look at the headline, not the details in the story. Another important characteristic of this dataset is that the relevant tickers in the story are extracted. Our dataset contains 82,643 headlines from to 2011-05-02 to 2018-12-28\n",
    "* **Yahoo finance website for the stock return:** (The return data can be obtained from other website such as yahoo finance)a\n",
    "* **kaggle**: labelled data of of news sentiments obtained for a classification based sentiment analysis model. This data may not be authentic and is used only for demonstration purpose in this case study. \n",
    "* Stock market lexicon created based on stock market conversations in microblogging services. The source of this lexicon is *Oliveira, Nuno, Paulo Cortez, and Nelson Areal. \"Stock market sentiment lexicon acquisition using microblogging data and statistical measures.\" Decision Support Systems 85 (2016): 62-73.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 2. Getting Started- Loading the data and python packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1. Loading the python packages\n",
    "\n",
    "As a first step we check if the additional packages needed are present, if not install them. These are checked separately as they aren't included in requirement.txt as they aren't used for all case studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import pip\n",
    "installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "required = {'nltk', 'spacy', 'textblob', 'backtrader'}\n",
    "missing = required - installedPackages\n",
    "if missing:\n",
    "    !pip install nltk==3.4\n",
    "    !pip install textblob==0.15.3\n",
    "    !pip install -U SpaCy==2.2.0\n",
    "    !python -m spacy download en_core_web_lg\n",
    "    !pip install backtrader==1.9.74.123    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "5d8fee34-f454-2642-8b06-ed719f0317e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tatsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#NLP libraries\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import nltk\n",
    "import warnings\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#Run the command python -m spacy download en_core_web_sm to download this\n",
    "#https://spacy.io/models\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "#Libraries for processing the news headlines\n",
    "from lxml import etree\n",
    "import json\n",
    "from io import StringIO\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pandas.tseries.offsets import BDay\n",
    "from scipy.stats.mstats import winsorize\n",
    "from copy import copy\n",
    "\n",
    "# Libraries for Classification for modeling the sentiments\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Keras package for the deep learning model for the sentiment prediction. \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# Load libraries\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "#Additional Libraries \n",
    "import json  \n",
    "import zipfile\n",
    "import os.path\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "## 2.2. Loading the stock price data\n",
    "\n",
    "The stock price data is loaded in this step from Yahoo Finance in this step. The loaded data is saved in csv for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-eec24dd44ad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mticker_yf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdf_ticker_return\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdf_ticker_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mticker_yf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdf_ticker_return\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\yfinance\\base.py\u001b[0m in \u001b[0;36mhistory\u001b[1;34m(self, period, interval, start, end, prepost, actions, auto_adjust, back_adjust, proxy, rounding, tz, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m                                \u001b[1;34m\"Our engineers are working quickly to resolve \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                                \"the issue. Thank you for your patience.\")\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# Work with errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\simplejson\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0mparse_constant\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             and not use_decimal and not kw):\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\simplejson\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_PY3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\simplejson\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mord0\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0xef\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'\\xef\\xbb\\xbf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m                 \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\simplejson\\scanner.py\u001b[0m in \u001b[0;36mscan_once\u001b[1;34m(string, idx)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expecting value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_scan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\simplejson\\scanner.py\u001b[0m in \u001b[0;36m_scan_once\u001b[1;34m(string, idx)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-Infinity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL','MSFT','AMZN','GOOG','FB','WMT','JPM','TSLA','NFLX','ADBE']\n",
    "start = '2010-01-01'\n",
    "end = '2018-12-31'\n",
    "df_ticker_return = pd.DataFrame()\n",
    "for ticker in tickers:    \n",
    "    ticker_yf = yf.Ticker(ticker)\n",
    "    if df_ticker_return.empty:\n",
    "        df_ticker_return = ticker_yf.history(start = start, end = end)\n",
    "        df_ticker_return['ticker']= ticker \n",
    "    else:\n",
    "        data_temp = ticker_yf.history(start = start, end = end)\n",
    "        data_temp['ticker']= ticker \n",
    "        df_ticker_return = df_ticker_return.append(data_temp)\n",
    "df_ticker_return.to_csv(r'Data\\Step2.2_ReturnData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the details of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticker_return.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the tickers and their return. In this next step, we clean the data make sure that the starting point is 2010 and the NAs in the data are dropped. Let us look at the news data now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data preparation into couple of steps as follows:\n",
    "* Loading and preprocessing the news data\n",
    "* Preparing the combined data\n",
    "\n",
    "\n",
    "#### <font color='red'>Note : Step 3.1 to 3.2 might be time consuming. Skip to step 3.3 and load the preprocessed data directly in case you want to avoid these steps. You can also start directly with step 4.4 in case you want to skip the model training and used the sentiments directly for the trading strategy.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "## 3.1 Loading and preprocessing News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news data is downloaded from the News RSS feed and the file is downloaded in the json format and the json files for different dates are kept under a zipped folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the content of the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(\"Data/Raw Headline Data.zip\", \"r\")\n",
    "testFile=z.namelist()[10]\n",
    "fileData= z.open(testFile).read() \n",
    "fileDataSample = json.loads(fileData)['content'][1:500]  \n",
    "fileDataSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the json format is not suitable for the algorithm. We need to get the news from the jsons and the following function is used for it. Regex becomes the vital part of this step. Regex can find a pattern in the raw, messy text and perform actions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function performs the json parsing given the \n",
    "def jsonParser(json_data): \n",
    "    xml_data = json_data['content']\n",
    "            \n",
    "    tree = etree.parse(StringIO(xml_data), parser=etree.HTMLParser())\n",
    "\n",
    "    headlines = tree.xpath(\"//h4[contains(@class, 'media-heading')]/a/text()\")\n",
    "    assert len(headlines) == json_data['count']\n",
    "\n",
    "    main_tickers = list(map(lambda x: x.replace('/symbol/', ''), tree.xpath(\"//div[contains(@class, 'media-left')]//a/@href\")))\n",
    "    assert len(main_tickers) == json_data['count']\n",
    "    final_headlines = [''.join(f.xpath('.//text()')) for f in tree.xpath(\"//div[contains(@class, 'media-body')]/ul/li[1]\")]\n",
    "    if len(final_headlines) == 0:\n",
    "        final_headlines = [''.join(f.xpath('.//text()')) for f in tree.xpath(\"//div[contains(@class, 'media-body')]\")]\n",
    "        final_headlines = [f.replace(h, '').split('\\xa0')[0].strip() for f,h in zip (final_headlines, headlines)]\n",
    "    return main_tickers, final_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonParser(json.loads(fileData))[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that using the json parser the news headlines are extracted from the complex html format. This format is good enough to be used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the ticker and the headlines from all the json files and put it in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None \n",
    "data_df_news = []\n",
    "ret = []\n",
    "ret_f = []\n",
    "with zipfile.ZipFile(\"Data/Raw Headline Data.zip\", \"r\") as z:\n",
    "    for filename in z.namelist(): \n",
    "        #print(filename)\n",
    "        try:               \n",
    "            #print('Running {}'.format(filename))\n",
    "            with z.open(filename) as f:  \n",
    "                data = f.read()  \n",
    "                json_data = json.loads(data)      \n",
    "            if json_data.get('count', 0)> 10:\n",
    "                #Step 1: Parse the News Jsons \n",
    "                main_tickers, final_headlines = jsonParser(json_data) \n",
    "                if len(final_headlines) != json_data['count']:\n",
    "                    continue\n",
    "                #Step 2: Prepare Future and Event Return and assign Future and Event return for each ticker. \n",
    "                file_date = filename.split('/')[-1].replace('.json', '')\n",
    "                file_date = date(int(file_date[:4]), int(file_date[5:7]), int(file_date[8:]))\n",
    "               #Step 3: Merge all the data in a data frame\n",
    "                df_dict = {'ticker': main_tickers,\n",
    "                           'headline': final_headlines,            \n",
    "                           'date': [file_date] * len(main_tickers)\n",
    "                           }\n",
    "                df_f = pd.DataFrame(df_dict)            \n",
    "                data_df_news.append(df_f)            \n",
    "        except:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the content of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_news=pd.concat(data_df_news)\n",
    "data_df_news.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data has the ticker, headlines and the date which will be used in the next step for combining with the return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "## 3.2 Preparing the combined data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we extract the event return, which is the return that corresponds to\n",
    "the event. We do this because at times the news is reported late and at other times it is\n",
    "reported after market close. Having a slightly wider window ensures that we capture\n",
    "the essence of the event. Event return in defined as follows:\n",
    "$ R_{t-1} + R_t + R_{t+1} $\n",
    "\n",
    "Where, $ R_{t-1} $, $ R_{t+1} $ are the return before and after the news data and $ R_{t} $ is the return on\n",
    "the day of the news (i.e. time t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the return\n",
    "df_ticker_return['ret_curr'] = df_ticker_return['Close'].pct_change()\n",
    "#Computing the event return\n",
    "df_ticker_return['eventRet'] = df_ticker_return['ret_curr'] + df_ticker_return['ret_curr'].shift(-1) + df_ticker_return['ret_curr'].shift(1)\n",
    "df_ticker_return.reset_index(level=0, inplace=True)\n",
    "df_ticker_return['date'] = pd.to_datetime(df_ticker_return['Date']).apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data in place we will prepare a combined dataframe which will have the news headlines mapped to the date, event Return and stock ticker. This dataframe will be used for further analysis for sentiment analysis model and for building the trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDataFrame = pd.merge(data_df_news, df_ticker_return, how='left', left_on=['date','ticker'], right_on=['date','ticker'])\n",
    "combinedDataFrame = combinedDataFrame[combinedDataFrame['ticker'].isin(tickers)]\n",
    "data_df = combinedDataFrame[['ticker','headline','date','eventRet','Close']]\n",
    "data_df = data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the data in a csv file to be used later, so that the data processing step can be skipped everytime we are looking into analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dropna().to_csv(r'Data\\Step3_NewsAndReturnData.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "## 3.3 Loading the preprocessed data\n",
    "#### Start from this step in case you dont want to run the previous preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r'Data\\Step3_NewsAndReturnData.csv', sep='|')\n",
    "data_df = data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.shape, data_df.ticker.unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we prepared a clean dataframe that has ticker, headline, event return, return for a given day and future return for 10 unique stock tickers with total 2759 rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 4 Evaluate Models for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go through following three different approaches of getting the sentiments for the news which we will use for building the trading strategy.\n",
    "\n",
    "* Predefined model-TextBlob package\n",
    "* Tuned Model-Classification algorithms and LSTM\n",
    "* Model based on financial lexicon\n",
    "\n",
    "We will also explore the difference between different ways of performing the sentiment analysis. Let us go through the steps. \n",
    "\n",
    "\n",
    "#### <font color='red'>Note : The steps 4.1-4.3 are time consuming steps. Skip to step 4.5 to load the pretrained sentiments and use them for further analysis and building trading strategy.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "## 4.1 - Predefined model-TextBlob package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texblob sentiment function is pretrained model based on Naïve-Bayes classification algorithm to convert a sentence to a numerical value of sentiment between -1 to +1 and map adjectives frequently found in movie reviews(source code: https://textblob.readthedocs.io/en/dev/_modules/textblob/en/sentiments.html) to sentiment polarity scores, ranging from -1 to +1 (negative ↔ positive) and a similar subjectivity score (objective ↔ subjective). We apply this on all headline articles. Let us compute the sentiment for all the headlines in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, touching their \\\n",
    "highest level in 14 months, after the U.S. government said a $25M glyphosate decision against the \\\n",
    "company should be reversed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text1).sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment polarity is a number between -1 (Very Negative) and +1 (Very Positive). We apply this on all headline we have in the data processed in the previous step. Let us compute the sentiment for all the headlines in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['sentiment_textblob'] = [TextBlob(s).sentiment.polarity for s in data_df['headline']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyse the scatterplot of the sentiments and the return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_df['sentiment_textblob'],data_df['eventRet'], alpha=0.5)\n",
    "plt.title('Scatter Between Event return and sentiments-all data')\n",
    "plt.ylabel('Event Return')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrlation = data_df['eventRet'].corr(data_df['sentiment_textblob'])\n",
    "print(corrlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is positive, which means that news with positive sentiments lead to positive return and is expected. However,the correlation isn't very high.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_stock  = data_df[data_df['ticker'] == 'AAPL']\n",
    "plt.scatter(data_df_stock['sentiment_textblob'],data_df_stock['eventRet'], alpha=0.5)\n",
    "plt.title('Scatter Between Event return and sentiments-AAPL')\n",
    "plt.ylabel('Event Return')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, from the results we can see that there isn't a strong correlation between the news and the sentiments. Also, there are a lot of sentiments centred around 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.5, assessments=[(['touching'], 0.5, 0.5, None)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, touching their highest level in 14 months, after the U.S. government said a $25M glyphosate decision against the company should be reversed.\"\n",
    "TextBlob(text).sentiment_assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the statement has a positive sentiment of .5 but looking at the words that give rise to the sentiments, the word \"touching\" and not \"high\" causes positive sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 4.2 - Supervised Learning-Classification algorithms and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we develop customised model for sentiment analysis, based on available labelled data. The label data for this is obtained from kaggle website. Let us look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_data = pd.read_csv(r'Data/LabelledNewsData.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/16/2020 5:25</td>\n",
       "      <td>$MMM fell on hard times but could be set to re...</td>\n",
       "      <td>MMM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         datetime                                           headline ticker  \\\n",
       "0  1/16/2020 5:25  $MMM fell on hard times but could be set to re...    MMM   \n",
       "\n",
       "   sentiment  \n",
       "0          0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9470 , 30\n"
     ]
    }
   ],
   "source": [
    "print(sentiments_data.shape[0],',', sentiments_data.ticker.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has headlines for the news across 30 different stocks, with total 9470 rows, and has sentiments lacelled 0 and 1. The headlines are already in almost cleaned. We perform the classification steps that we discussed in chapter 6, using the classification model development python template discussed in that chapter. \n",
    "\n",
    "In order to run a supervised learning model, we first need to convert the news headlines into feature representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word-embedding\n",
    "all_vectors = pd.np.array([pd.np.array([token.vector for token in nlp(s) ]).mean(axis=0)*pd.np.ones((300)) \\\n",
    "                           for s in sentiments_data['headline']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the independent variable we train the classification model in a similar manner as discussed in chapter 6. We first divide the data into training set and test set and futher run the key classification models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out validation dataset for the end\n",
    "Y= sentiments_data[\"sentiment\"]\n",
    "X = all_vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "validation_size = 0.3\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "# test options for classification\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# spot check the algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC()))\n",
    "#Neural Network\n",
    "models.append(('NN', MLPClassifier()))\n",
    "#Ensable Models \n",
    "models.append(('RF', RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all the classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "kfold_results = []\n",
    "test_results = []\n",
    "train_results = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    #print(msg)\n",
    "   # Full Training period\n",
    "    res = model.fit(X_train, Y_train)\n",
    "    train_result = accuracy_score(res.predict(X_train), Y_train)\n",
    "    train_results.append(train_result)\n",
    "    \n",
    "    # Test results\n",
    "    test_result = accuracy_score(res.predict(X_test), Y_test)\n",
    "    test_results.append(test_result)    \n",
    "    \n",
    "    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n",
    "    print(msg)\n",
    "    print(confusion_matrix(res.predict(X_test), Y_test))\n",
    "    #print(classification_report(res.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "from matplotlib import pyplot\n",
    "fig = pyplot.figure()\n",
    "ind = np.arange(len(names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\n",
    "pyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the NN model is the best performer with the a training accuracy of 99% and test accuracy of 93%. The performance of Random forest, SVM and Logistic regression are good as well. CART and KNN don't perform as good as other models. CART has higher overfitting as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet, we used the some of the model function which look at the individual works in the sentence for training. However, for NLP, RNN based model are preferred over other machine learning models as RNN stores the information for current feature as well neighboring features for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(sentiments_data['headline'])\n",
    "sequences = tokenizer.texts_to_sequences(sentiments_data['headline'])\n",
    "X_LSTM = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the data into the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_LSTM = sentiments_data[\"sentiment\"]\n",
    "X_train_LSTM, X_test_LSTM, Y_train_LSTM, Y_test_LSTM = train_test_split(X_LSTM, \\\n",
    "                       Y_LSTM, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, we used Keras library to build a neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def create_model(input_length=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(20000, 300, input_length=50))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model    \n",
    "model_LSTM = KerasClassifier(build_fn=create_model, epochs=3, verbose=1, validation_split=0.4)\n",
    "model_LSTM.fit(X_train_LSTM, Y_train_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result_LSTM = accuracy_score(model_LSTM.predict(X_train_LSTM), Y_train_LSTM)\n",
    "# Test results\n",
    "test_result_LSTM = accuracy_score(model_LSTM.predict(X_test_LSTM), Y_test_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the accuracy and confusion metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_result_LSTM,test_result_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(model_LSTM.predict(X_test_LSTM), Y_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results.append(train_result_LSTM);test_results.append(test_result_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.append(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "from matplotlib import pyplot\n",
    "fig = pyplot.figure()\n",
    "ind = np.arange(len(names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\n",
    "pyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the LSTM model has the best performance in the test set as compared to all other models that we saw before. Let us use LSTM model for the computation of the sentiments in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_LSTM = tokenizer.texts_to_sequences(data_df['headline'])\n",
    "X_LSTM = pad_sequences(sequences_LSTM, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_LSTM = model_LSTM.predict(X_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['sentiment_LSTM'] = Y_LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrlation = data_df['eventRet'].corr(data_df['sentiment_LSTM'])\n",
    "print(corrlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "## 4.3 - Unsupervised - Model based on financial lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicons are special dictionaries or vocabularies that have been created for analyzing sentiments.\n",
    "VADER (Valence Aware Dictionary for Entiment Reasoning) is a pre-built sentiment analysis model included in the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock market lexicon\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stock_lex = pd.read_csv('Data\\LexiconData.csv')\n",
    "stock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\n",
    "stock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\n",
    "stock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\n",
    "stock_lex_scaled = {}\n",
    "for k, v in stock_lex.items():\n",
    "    if v > 0:\n",
    "        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n",
    "    else:\n",
    "        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n",
    "\n",
    "final_lex = {}\n",
    "final_lex.update(stock_lex_scaled)\n",
    "final_lex.update(sia.lexicon)\n",
    "sia.lexicon = final_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of extracting sentiment score for a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"AAPL has little competition from any of the tech companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"AAPL is trading higher after reporting its October sales rose 12.6% M/M. It has seen a 20%+ jump in orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0297"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TextBlob(text).sentiment_assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the sentiment for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2759,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiments = pd.np.array([sia.polarity_scores(s)['compound'] for s in data_df['headline']])\n",
    "vader_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['sentiment_lex'] = vader_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1047785005800468\n"
     ]
    }
   ],
   "source": [
    "corrlation = data_df['eventRet'].corr(data_df['sentiment_lex'])\n",
    "print(corrlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_df['sentiment_lex'],data_df['eventRet'], alpha=0.5)\n",
    "plt.title('Scatter Between Event return and sentiments-all data')\n",
    "plt.ylabel('Event Return')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don's see many high returns for lower sentiments, but the data may not be very clear. Let us look at the result for one of the stock tickers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_stock  = data_df[data_df['ticker'] == 'AMZN']\n",
    "plt.scatter(data_df_stock['sentiment_lex'],data_df_stock['eventRet'], alpha=0.5)\n",
    "plt.title('Scatter Between Event return and sentiments-AMZN')\n",
    "plt.ylabel('Event Return')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a positive correlation between the event return and sentiments. We will look deeper into the comparison of different types of the sentiment analysis in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(r'Data\\Step4_DataWithSentimentsResults.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.4'></a>\n",
    "## 4.4 Exploratory Data Analysis and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r'Data\\Step4_DataWithSentimentsResults.csv', sep='|')\n",
    "data_df = data_df[data_df['ticker'].isin(tickers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the sample headlines and the the sentiments from three different methodology, followed by the analysis using visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>headline</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_lex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>Netflix (NFLX +1.1%) shares post early gains after Citigroup ups its rating to Buy and lifts its price target to $300 from $245. U.S. revenue growth is sustainable, Citi says, \"with a path to 50M subscribers by 2013,\" adding that NFLX has little competition in price, selection and  convenience; mass market adoption of tablets will help, and the mass-market adoption phase is still to come.</td>\n",
       "      <td>-0.04375</td>\n",
       "      <td>0.8575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  \\\n",
       "1   NFLX   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                  headline  \\\n",
       "1  Netflix (NFLX +1.1%) shares post early gains after Citigroup ups its rating to Buy and lifts its price target to $300 from $245. U.S. revenue growth is sustainable, Citi says, \"with a path to 50M subscribers by 2013,\" adding that NFLX has little competition in price, selection and  convenience; mass market adoption of tablets will help, and the mass-market adoption phase is still to come.   \n",
       "\n",
       "   sentiment_textblob  sentiment_lex  \n",
       "1            -0.04375         0.8575  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_new_df_stock=data_df[data_df['ticker']== 'NFLX'][['ticker','headline','sentiment_textblob','sentiment_lex']]\n",
    "from pandas import option_context\n",
    "\n",
    "with option_context('display.max_colwidth', 400):\n",
    "    display(data_new_df_stock.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'LinkedIn (LNKD) could have an exceptional drop in its price over the coming months'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>News</th>\n",
       "      <th>sentiment_financial</th>\n",
       "      <th>sentiment_moveies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LNKD</td>\n",
       "      <td>LinkedIn (LNKD) could have an exceptional drop in its price over the coming months</td>\n",
       "      <td>-0.1945</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  \\\n",
       "0   LNKD   \n",
       "\n",
       "                                                                                 News  \\\n",
       "0  LinkedIn (LNKD) could have an exceptional drop in its price over the coming months   \n",
       "\n",
       "   sentiment_financial  sentiment_moveies  \n",
       "0              -0.1945               0.66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [['LNKD','LinkedIn (LNKD) could have an exceptional drop in its price over the coming months',-.1945, 0.66]]\n",
    " \n",
    "\n",
    "df = pd.DataFrame(data, columns = ['ticker', 'News', 'sentiment_financial','sentiment_moveies'])\n",
    "\n",
    "with option_context('display.max_colwidth', 200):\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1945"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['exceptional'], 0.6666666666666666, 1.0, None)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).sentiment_assessments.assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at one of the headlines, the sentiment from this sentence is positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data_df[['sentiment_textblob','sentiment_LSTM','sentiment_lex','eventRet']].dropna(axis=0).corr()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title('Correlation Matrix')\n",
    "sns.heatmap(correlation[['eventRet']], vmax=1, annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the sentiments have positive relationship with the return which is intuitive and expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = []\n",
    "for ticker in data_df['ticker'].unique():\n",
    "    data_new_df_stock=data_df[data_df['ticker']==ticker]\n",
    "    #Only look for the stocks with sufficient data\n",
    "    if data_new_df_stock.shape[0] > 40 : \n",
    "        corr_textblob= data_new_df_stock['eventRet'].corr(data_new_df_stock['sentiment_textblob'])    \n",
    "        corr_LSTM = data_new_df_stock['eventRet'].corr(data_new_df_stock['sentiment_LSTM'])\n",
    "        corr_lex = data_new_df_stock['eventRet'].corr(data_new_df_stock['sentiment_lex'])\n",
    "        corr_data.append([ticker,corr_textblob, corr_LSTM, corr_lex])\n",
    "        #print(ticker,corr_vader, corr_LSTM, corr_textblob)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(corr_data, columns =  ['ticker','corr_textblob','corr_LSTM','corr_lex'])  \n",
    "corr_df=corr_df.set_index('ticker')\n",
    "corr_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_df.to_csv(\"correlation.csv\")     \n",
    "#tickers = ['AAPL','MSFT','AMZN','GOOG','BABA','FB','WMT','V','JPM','TSLA']\n",
    "corr_df.loc[tickers].plot.bar(figsize = (10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the chart the corrlation from the lexicon methodology is highest across all the stock tickers, which corraborates the conclusion from the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tickers = corr_df.nlargest(5,'corr_lex').index\n",
    "for ticker in tickers[2:4]:\n",
    "    data_df_stock  = data_df[data_df['ticker'] == ticker]\n",
    "    fig = plt.figure(figsize=(14, 4), constrained_layout=False)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(data_df_stock['sentiment_lex'],data_df_stock['eventRet'], alpha=0.5)\n",
    "    plt.title(ticker + '-Scatter Between Event return and sentiments-lexicon')\n",
    "    plt.ylabel('Event Return')\n",
    "    plt.xlabel('Sentiments-Lexicon')\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(data_df_stock['sentiment_textblob'],data_df_stock['eventRet'], alpha=0.5)\n",
    "    plt.title(ticker + '-Scatter Between Event return and sentiments-textblob')\n",
    "    plt.ylabel('Event Return')\n",
    "    plt.xlabel('Sentiments-Textblob')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon based sentiments on the left definitely shows a positive relationship between the sentiments and return. We use lexicon based sentiments for the trading strategy in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# 5. Model Evaluation- Building a Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment data can be used in different ways for the trading strategy. Sentiment scores can be used as a directional signal and ideally create a long-short portfolio, by buying the stocks with positive score and selling the stocks with negative score. The sentiments can also be used as additional features over and above other features(such as correlated stocks, technical indicators) in a supervised learning model to predict the price or come up with a trading strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trading strategy in this case study we buy and sell stock as per the current stock sentiments : \n",
    "* Buy a stock when the change in sentiment score (Current sentiment score - previous sentiment score) is greater than .5 and sell a stock when the change in sentiment score is less than -.5.\n",
    "* Additionally, we check for 15 days moving average while buying and selling and buy or sell in a unit of 100. \n",
    "\n",
    "Obviusly, there can be many ways to create the trading strategy based in sentiments, by varying the threshold, or changing the number of units based on the initial cash available.\n",
    "\n",
    "We use lexicon based sentiments for the trading strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## 5.1. Setting up the strategy \n",
    "Here we use backtrader which is a Python based API for writing and backtesting trading strategy. . Backtrader allows you to focus on writing reusable trading strategies, indicators and analyzers instead of having to spend time building infrastructure. we have a convenient framework to backtest and write our trading strategy. We used the Quickstart code in the docs (i.e. Refer to https://www.backtrader.com/docu/quickstart/quickstart/) as a base and modified it to include the sentiment scores.\n",
    "\n",
    "We imlement a simple strategy to buy if the previous day’s sentiment score increases by 0.5 from the last day and sell if it decreases by 0.5.\n",
    "\n",
    "\n",
    "The following function contains two classes: \n",
    "* Sentiment:\n",
    "* SentimentStrat: The \"next\" function of this class implements the actual trading strategy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import backtrader.analyzers as btanalyzers\n",
    "\n",
    "class Sentiment(bt.Indicator):\n",
    "    lines = ('sentiment',)\n",
    "    plotinfo = dict(\n",
    "        plotymargin=0.5,\n",
    "        plothlines=[0],\n",
    "        plotyticks=[1.0, 0, -1.0])\n",
    "    \n",
    "    def next(self):\n",
    "        self.sentiment = 0.0\n",
    "        self.date = self.data.datetime\n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment        \n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        self.lines.sentiment[0] = self.sentiment\n",
    "\n",
    "\n",
    "class SentimentStrat(bt.Strategy):\n",
    "    params = (\n",
    "        ('period', 15),\n",
    "        ('printlog', True),\n",
    "    )\n",
    "\n",
    "    def log(self, txt, dt=None, doprint=False):\n",
    "        ''' Logging function for this strategy'''\n",
    "        if self.params.printlog or doprint:\n",
    "            dt = dt or self.datas[0].datetime.date(0)\n",
    "            print('%s, %s' % (dt.isoformat(), txt))\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "        # Keep track of pending orders\n",
    "        self.order = None\n",
    "        self.buyprice = None\n",
    "        self.buycomm = None\n",
    "        self.sma = bt.indicators.SimpleMovingAverage(\n",
    "            self.datas[0], period=self.params.period)\n",
    "        self.date = self.data.datetime\n",
    "        self.sentiment = None\n",
    "        Sentiment(self.data)\n",
    "        self.plotinfo.plot = False\n",
    "        \n",
    "        \n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
    "            return\n",
    "        \n",
    "        # Check if an order has been completed\n",
    "        # Attention: broker could reject order if not enough cash\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    'BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                    (order.executed.price,\n",
    "                     order.executed.value,\n",
    "                     order.executed.comm))\n",
    "                self.buyprice = order.executed.price\n",
    "                self.buycomm = order.executed.comm\n",
    "            else:  # Sell\n",
    "                self.log('SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n",
    "                         (order.executed.price,\n",
    "                          order.executed.value,\n",
    "                          order.executed.comm))\n",
    "                \n",
    "            self.bar_executed = len(self)     \n",
    "            \n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log('Order Canceled/Margin/Rejected')\n",
    "            \n",
    "        # Write down: no pending order\n",
    "        self.order = None\n",
    "        \n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "\n",
    "        self.log('OPERATION PROFIT, GROSS %.2f, NET %.2f' %\n",
    "                 (trade.pnl, trade.pnlcomm))\n",
    "    \n",
    "    ### Main Strat ###\n",
    "    def next(self):        \n",
    "        date = bt.num2date(self.date[0]).date()\n",
    "        prev_sentiment = self.sentiment\n",
    "        if date in date_sentiment:\n",
    "            self.sentiment = date_sentiment[date]\n",
    "        \n",
    "        # Check if an order is pending. if yes, we cannot send a 2nd one\n",
    "        if self.order:\n",
    "            return       \n",
    "        # If not in the market and previous sentiment not none\n",
    "        if not self.position and prev_sentiment:\n",
    "            # buy if current close more than sma AND sentiment increased by >= 0.5\n",
    "            if self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\n",
    "                self.log('Previous Sentiment %.2f, New Sentiment %.2f BUY CREATE, %.2f' % (prev_sentiment, self.sentiment, self.dataclose[0]))                \n",
    "                self.order = self.buy()\n",
    "                \n",
    "        # Already in the market and previous sentiment not none\n",
    "        elif prev_sentiment:\n",
    "            # sell if current close less than sma AND sentiment decreased by >= 0.5\n",
    "            if self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\n",
    "                self.log('Previous Sentiment %.2f, New Sentiment %.2f SELL CREATE, %.2f' % (prev_sentiment, self.sentiment, self.dataclose[0]))                \n",
    "                self.order = self.sell()\n",
    "\n",
    "    def stop(self):\n",
    "        self.log('(MA Period %2d) Ending Value %.2f' %\n",
    "                 (self.params.period, self.broker.getvalue()), doprint=True)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for running the trading strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a generic function to run the strategy for any stock. We specified the “ticker” stock feeds to be pulled from Yahoo Finance, set an initial amount of $100,000, a fixed size of 100 lots per trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strategy(ticker, start, end):\n",
    "    print(ticker)    \n",
    "    ticker = yf.Ticker(ticker)\n",
    "    df_ticker = ticker.history(start = start, end = end)\n",
    "    \n",
    "    cerebro = bt.Cerebro()\n",
    "    # Add the data\n",
    "    cerebro.addstrategy(SentimentStrat)        \n",
    "    data = bt.feeds.PandasData(dataname=df_ticker)\n",
    "    cerebro.adddata(data)    \n",
    "    start = 100000.0\n",
    "    cerebro.broker.setcash(start)   \n",
    "    cerebro.addsizer(bt.sizers.FixedSize, stake=100)\n",
    "    print('Starting Portfolio Value: %.2f' % start)    \n",
    "    plt.rcParams['figure.figsize']=[10,6]\n",
    "    plt.rcParams[\"font.size\"]=\"12\"\n",
    "    cerebro.run() \n",
    "    cerebro.plot(volume=False, iplot=True, plotname= ticker)\n",
    "    end = cerebro.broker.getvalue()\n",
    "    print('Start Portfolio value: %.2f\\nFinal Portfolio Value: %.2f\\nProfit: %.2f\\n' \\\n",
    "          % (start, end, end - start))\n",
    "    return float(df_ticker['Close'][0]), (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## 5.2. Results for Individual Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First running the strategy for google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'GOOG'\n",
    "date_sentiment=data_df[data_df['ticker'].isin([ticker])]\n",
    "date_sentiment=date_sentiment[['date','sentiment_lex']]\n",
    "date_sentiment['date']=pd.to_datetime(date_sentiment['date'], format='%Y-%m-%d').dt.date\n",
    "date_sentiment=date_sentiment.set_index('date')['sentiment_lex']\n",
    "date_sentiment=date_sentiment.to_dict()\n",
    "run_strategy(ticker, start = '2012-01-01', end = '2018-12-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show an overall profit of 49719.00. The chart is divided into three panels. \n",
    "\n",
    "* Top Panel : Top panel is the cash value observer which as the name implies keeps track of the Cash and total portolio Value (including cash) during the life of the backtesting run. As we can see that we started with 100000.00 and final value at the end is 149719.00 as shown in this panel.\n",
    "\n",
    "* Second Panel : This panel is Trade Observer which shows, at the end of a trade, the actual Profit and Loss. A trade is defined as opening a position and taking the position back to 0 (directly or crossing over from long to short or short to long). Five out of eight actions of buy and sell are profitable for the strategy. \n",
    "\n",
    "* Third Panel : This panel is Buy Sell observer which plots (on top of the prices) where buy and sell operations have taken place. In general we see that (specially around 2018) the buy action takes place when the stock price is increasing and the sell action takes place when the stock price has started declining.   \n",
    "\n",
    "* Bottom Panel : This panel shows the sentiment score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose one of the days on which buy was triggered and we look at the news on that data and previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOG_ticker= data_df[data_df['ticker'].isin([ticker])]\n",
    "New= list(GOOG_ticker[GOOG_ticker['date'] ==  '2015-07-17']['headline'])\n",
    "Old= list(GOOG_ticker[GOOG_ticker['date'] ==  '2015-07-16']['headline'])\n",
    "print(\"Current News:\",New,\"\\n\\n\",\"Previous News:\", Old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the News on the current day has positive sentiment as compared to the news at the previous day causing the buy to be triggered. Now, we run the strategy for Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'FB'\n",
    "date_sentiment=data_df[data_df['ticker'].isin([ticker])]\n",
    "date_sentiment=date_sentiment[['date','sentiment_lex']]\n",
    "date_sentiment['date']=pd.to_datetime(date_sentiment['date'], format='%Y-%m-%d').dt.date\n",
    "date_sentiment=date_sentiment.set_index('date')['sentiment_lex']\n",
    "date_sentiment=date_sentiment.to_dict()\n",
    "run_strategy(ticker, start = '2012-01-01', end = '2018-12-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Panel : The CashValue shows overall profit of 8041.00.\n",
    "\n",
    "Second Panel : A Trade Observer shows that six out of 7 actions are profitable. \n",
    "\n",
    "Third Panel : A BuySell observer shows that in general the buy action takes place when the stock price is increasing and the sell action takes place when the stock price has started declining.   \n",
    "\n",
    "Forth Panel : It shows high number of positive sentiment for the FB around 2013 and 2014 period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.3'></a>\n",
    "## 5.3. Results for Multiple Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tickers = {}\n",
    "for ticker in tickers:    \n",
    "    date_sentiment=data_df[data_df['ticker'].isin([ticker])]\n",
    "    date_sentiment=date_sentiment[['date','sentiment_lex']]\n",
    "    date_sentiment['date']=pd.to_datetime(date_sentiment['date'], format='%Y-%m-%d').dt.date\n",
    "    date_sentiment=date_sentiment.set_index('date')['sentiment_lex']\n",
    "    date_sentiment=date_sentiment.to_dict()\n",
    "    results_tickers[ticker] = run_strategy(ticker, start = '2012-01-01', end = '2018-12-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_tickers).set_index([pd.Index([\"PerUnitStartPrice\", 'StrategyProfit'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy yield an overall profit be for all the stock. Now we run the strategy varying the time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.4'></a>\n",
    "## 5.4. Varying the strategy time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we run the strategy for differnt time periods and look at the results. We first run it for the time period between 2012 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tickers = {}\n",
    "for ticker in tickers:  \n",
    "    date_sentiment=data_df[data_df['ticker'].isin([ticker])]\n",
    "    date_sentiment=date_sentiment[['date','sentiment_lex']]\n",
    "    date_sentiment['date']=pd.to_datetime(date_sentiment['date'], format='%Y-%m-%d').dt.date\n",
    "    date_sentiment=date_sentiment.set_index('date')['sentiment_lex']\n",
    "    date_sentiment=date_sentiment.to_dict()\n",
    "    results_tickers[ticker] = run_strategy(ticker, start = '2012-01-01', end = '2014-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_tickers).set_index([pd.Index([\"StockPriceBeginning\", 'StrategyProfit'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy yield an overall profit for all the stocks except two. Now we run the strategy between 2016 and 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tickers = {}\n",
    "for ticker in tickers: \n",
    "    date_sentiment=data_df[data_df['ticker'].isin([ticker])]\n",
    "    date_sentiment=date_sentiment[['date','sentiment_lex']]\n",
    "    date_sentiment['date']=pd.to_datetime(date_sentiment['date'], format='%Y-%m-%d').dt.date\n",
    "    date_sentiment=date_sentiment.set_index('date')['sentiment_lex']\n",
    "    date_sentiment=date_sentiment.to_dict()\n",
    "    results_tickers[ticker] = run_strategy(ticker, start = '2016-01-01', end = '2018-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_tickers).set_index([pd.Index([\"PerUnitStartPrice\", 'StrategyProfit'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a good performance of the sentiment based strategy across all the stocks except AAPL and we can conclude that our sentiment based strategy performs quite well on different time period. The strategy can be further be tweaked to modify the threshold, order size. Additional metrics such as sharpe ratio and maximum drawdown can also be used to understand the performance of the stratefy. The sentiments can also be used along with the other features such as correlated variables and technical indicators for prediction.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    " We performed a comparison of the models and concluded that one of the most important step is training the model for sentiment analysis is training it using the domain-specific vocabulary. \n",
    "\n",
    "We further used the sentiments as signals to develop different trading strategy. This initial result suggests that the model trained on a financial lexicon based sentiments could prove a viable model for a trading strategy. \n",
    "\n",
    "Additional improvements to this can be made by using more complex pre-trained sentiment analysis models such as Bert by google or diffeent pre-trained NLP models available in open source platforms. Existing NLP libraries fill in some of the pre-processing and encoding steps to allow us to focus on the inference step.\n"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 206,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
